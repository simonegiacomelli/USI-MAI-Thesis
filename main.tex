%v1.16

\documentclass[11pt, mscthesis]{usiinfthesis}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc} % Output font encoding for international characters
\usepackage{svg}
\usepackage[english]{babel}
\usepackage{lipsum}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{subfig}
\lstset{columns=fixed, basicstyle=\ttfamily, basewidth=0.5em}

% \RequirePackage{scrhack} % Loads fixes for various packages

\graphicspath{{./images/}{./images2/}}
\DeclareGraphicsExtensions{*.svg,.png,.pdf}

\newcommand{\vect}[1]{\boldsymbol{#1}}
%%%%%
\makeatletter
\newcommand{\labeltextbf}[3][]{%
    \@bsphack%
    \csname phantomsection\endcsname% in case hyperref is used
    \def\tst{#1}%
    \def\labelmarkup{\textbf}% How to markup the label itself
    %\def\refmarkup{\labelmarkup}% How to markup the reference
    \def\refmarkup{}%
    \ifx\tst\empty\def\@currentlabel{\refmarkup{#2}}{\label{#3}}%
    \else\def\@currentlabel{\refmarkup{#1}}{\label{#3}}\fi%
    \@esphack%
    \labelmarkup{#2}% visible printed text.
}
\makeatother

%%%%%
\lstdefinelanguage{algebra}
{morekeywords={import,sort,constructors,observers,transformers,axioms,if,
else,end},
sensitive=false,
morecomment=[l]{//s},
}
\DeclareUnicodeCharacter{0301}{*************************************}


\usepackage{subfiles}
\usepackage{biblatex}
\addbibresource{biblio.bib}

\title{Using Deep Learning to Identify Technical Debt} %compulsory
\specialization{Major in Artificial Intelligence}%optional
\subtitle{Levereging Self Admitted Technical Debt} %optional 
\author{Simone Giacomelli} %compulsory
\begin{committee}
\advisor{Prof. Dr.}{Gabriele Bavota}{} %compulsory
\coadvisor{Dr.}{Csaba Nagy}{}{} %optional
\end{committee}
\Day{1} %compulsory
\Month{January} %compulsory
\Year{2021} %compulsory, put only the year
\place{Lugano} %compulsory

\dedication{To the part of you that will always\\learn new things and strive to be better}

%\openepigraph{Someone said \dots}{Someone} %optional

%\makeindex %optional, also comment out \theindex at the end

\begin{document}

\maketitle %generates the titlepage, this is FIXED

\frontmatter %generates the frontmatter, this is FIXED

\begin{abstract}
% Composed of four parts: \\
% a: context\\
% b: problem\\
% c: proposed solution\\
% d: results\\
% \\
% max 400 words.
The `technical debt' metaphor describes a misalignment between the appropriate solution to a problem and the non-optimal actual implementation. Software developers accumulate technical debt when they choose speed at the expense of quality and correctness. As the debt metaphor suggests, there are multiple elements involved; two of them are: gain and interest. The gain is a shorter time to market and lower initial costs. The interest is all the additional effort caused by choosing the easy but limited solution, i.e. it's harder to make changes on suboptimal code implementation. When the debt is not paid in a timely manner the interest can crush the evolution capabilities of the project itself. Researcher agrees that it's important to track, manage and repay technical debt to avoid dire consequences.

Developers, while introducing technical debt, can decide to leave a source code comment to document what they are doing; this is called self-admitted technical debt (SATD). A SATD is an acknowledgement that something needs to be fixed.

In this thesis we propose a technical debt classifier fueled by the SATD harvested from 245,243 GitHub Java projects. We detail the process of the creation of the dataset. We explain how to extract features from a source snippet; these features are learned by the network and, also using an attention mechanism, encoded in a fixed-length vector. The last layer of the deep learning model finally classifies the snippet as TD-free or TD-affected.  

In this empirical study we assess, using quantitative and qualitative research, the accuracy of the classifier and analyze how the prediction confidence influences it.

The results on multiple datasets show a precision ranging between 67\% and 71\%, and a F1-score between 61\% and 66\%. Our quantitative analysis on the prediction confidence score shows that we can significantly increase the precision at the expanse of lowering the recall. 
We explore quantitative and qualitative findings in both correct and incorrect prediction; we show patterns successfully learned by the network (empty exception block, magic constant and return null) and we also show why specific predictions fail. We conducted an experiment to optimize the hyperparameters of the model using a distributed computing grid search.


% The results on the confidence level analysis show interesting results: when filtering for a confidence level greater than 0.9 we measured a gain on precision, from 71\% to 78\%; the excluded samples shows their effect also on the recall that goes from 62\% down to 52\%. When using a dataset with bigger code snippets and confidence greater than 0.9 we discard 98\% of the test predictions; however the precision is high as 99\% and the recall drops to 10\%, both due to the (correct and incorrect) discarded predictions.


\end{abstract}


\begin{acknowledgements}
I would like to thank my advisors Prof. Dr. Gabriele Bavota and Dr. Csaba Nagy. They helped me more than they can imagine. Their insightful comments and professionalism contributed immensely to make this thesis an amazing and rewarding project.
\\
\\
A huge thanks to all the people that make my university tick. I'm grateful to all the staff of Universit\`a della Svizzera Italiana, you inspired me on the journey that led me to join the Academic Senate and the Council of the Faculty of Informatics as student representative. I met wonderful people and you all made it an invaluable learning experience. You will always have a special place in my heart. USI is a young and beautiful university, I hope you are enjoying it like I did.

%thanks family

%thanks friends

\end{acknowledgements}

\tableofcontents 
\listoffigures %optional
\listoftables %optional

\mainmatter

\input{1_intro}
\input{2_art}
\input{3_using}
\input{4_empirical}
\input{5_results}
\input{6_threats}
\input{7_conclusion}

%%%%%%% commented appendix

\appendix

\input{appendix}
% \appendix %optional, use only if you have an appendix
% \chapter{Some retarded material}
% \section{It's over\dots}
% \lipsum 

\nocite{*}
\backmatter

%%%%%%% commented glossary
%\chapter{Glossary} %optional

%\bibliographystyle{alpha}
%\bibliographystyle{dcu}
%\bibliographystyle{plainnat}
%\bibliography{biblio}



\printbibliography
%\cleardoublepage
%\theindex %optional, use only if you have an index, must use
%\makeindex in the preamble
\end{document}